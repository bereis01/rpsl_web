import os
import json
import pickle
import argparse
from . import context
from shared.storage import ObjStr
from . import cleaning, restructuring, parsing, analysis

# Instantiates connection to storage
rpslyzer_output_path = os.getenv("DATA_INPUT_PATH")
objects_path = os.getenv("DATA_OUTPUT_PATH")

if not os.path.isdir(objects_path):
    os.mkdir(objects_path)
storage = ObjStr(objects_path)

# Parses command line arguments
parser = argparse.ArgumentParser(
    prog="rpslweb data processor",
    description="Processes further the data generated by RPSLyzer",
    epilog="For use with rpslweb's backend",
)
parser.add_argument("-o", "--organize", action="store_true")
parser.add_argument("-r", "--restructure", action="store_true")
parser.add_argument("-c", "--clean", action="store_true")
parser.add_argument("-p", "--parse", action="store_true")
parser.add_argument("-x", "--preprocess", action="store_true")
parser.add_argument("-a", "--analyze", action="store_true")
parser.add_argument("-f", "--full", action="store_true")
args = parser.parse_args()


# Organizes the raw output of rpslyzer
def organize():
    # Unites the output from rpslyzer
    input_file = open(f"{rpslyzer_output_path}/0.json")
    united = json.load(input_file)
    input_file.close()
    for i in range(1, 48):
        input_file = open(f"{rpslyzer_output_path}/{i}.json")
        data = json.load(input_file)
        for key in united.keys():
            united[key] = united[key] | data[key]
        input_file.close()
    output_file = open(f"{rpslyzer_output_path}/full.json", "w")
    json.dump(united, output_file)
    output_file.close()

    # Converts every entry of the data to string
    f = open(f"{rpslyzer_output_path}/full.json")
    data = json.load(
        f,
        parse_float=lambda x: str(x),
        parse_int=lambda x: str(x),
        parse_constant=lambda x: str(x),
    )
    if not os.path.isdir(f"{objects_path}/raw"):
        os.mkdir(f"{objects_path}/raw")
    for key in data.keys():
        with open(f"{objects_path}/raw/{key}", "wb") as f:
            pickle.dump(data[key], f)
    f.close()


# Function for restructuring rpslyzers output
def restructure():
    # Opens the data
    data = storage.get_bucket("raw")

    # Calls the restructure module
    # Inplace modifications on data
    restructuring.process(data)

    # Writes back the cleaned data
    storage.set_bucket("preprocessed", data)


# Function for cleaning the restructured data
def clean():
    # Opens the data
    data = storage.get_bucket("preprocessed")

    # Calls the cleaning module
    # Inplace modifications on data
    cleaning.process(data)

    # Writes back the cleaned data
    storage.set_bucket("preprocessed", data)


# Function for parsing rpslyzers output
def parse():
    # Reads the output from RPSLyzer
    # Converts all numeric data types to string
    print("\n***STARTING***\n")
    print("Reading input data...", end="", flush=True)
    data = storage.get_bucket("preprocessed")
    print("DONE")

    # Processes each key of RPSLyzer's output
    rpslyzer_keys = [
        "aut_nums",
        "as_sets",
        "route_sets",
        "peering_sets",
        "filter_sets",
        "as_routes",
    ]
    for key in rpslyzer_keys:
        print(f"Processing '{key}'...", end="", flush=True)
        parsing.process(key, data[key], storage)
        print("DONE")

    print("\n***FINISHING***\n")
    del data


# Function for generating more data on top of RPSLyzer
def analyze():
    print("\n***STARTING***\n")

    # Processes the 'aut_nums' key
    print("Processing relationships...", end="", flush=True)
    analysis.process_relationships(storage)

    print("DONE")
    print("\n***FINISHING***\n")


# Executes actions based on arguments
if args.full:
    organize()
    restructure()
    clean()
    parse()
    analyze()
    quit()
if args.preprocess:
    restructure()
    clean()
    parse()
    quit()
if args.organize:
    organize()
if args.restructure:
    restructure()
if args.clean:
    clean()
if args.parse:
    parse()
if args.analyze:
    analyze()
